# ============================================================================
# RasQberry Pi Image Build Workflow
# ============================================================================
# Purpose: Automates the building of custom Raspberry Pi OS images for the 
#          RasQberry quantum computing education platform.
#
# Key Features:
# - Automated versioning with semantic versioning support
# - Intelligent caching system for faster development builds
# - Automatic GitHub releases with built images
# - Memory and disk optimization for GitHub Actions runners
# - Support for both development and production builds
# - A/B boot images for over-the-air updates
#
# Trigger Conditions:
# - Manual workflow dispatch (with optional version override)
# - Automatic on push to dev* branches
# ============================================================================

# ============================================================================
# BUILD ARCHITECTURE OVERVIEW
# ============================================================================
# 
# ┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
# │ Version Manager │────▶│ Release Creator │────▶│  Image Builder  │
# └─────────────────┘     └─────────────────┘     └─────────────────┘
#         │                        │                        │
#         ▼                        ▼                        ▼
#   Update VERSION          Create GitHub          Build Pi Image
#   Create Git Tag          Release with           - Use cache (dev)
#   Push Changes            Changelog              - Delete+refresh (main)
#                                                   - Upload Asset
#
# CACHING STRATEGY:
# 
# First Build:          Subsequent Builds:       Monthly/Forced:
# ┌──────────┐         ┌──────────┐            ┌──────────┐
# │ Stage 0  │         │ Stage 0  │            │ Stage 0  │
# │ Stage 1  │ Build   │ Stage 1  │ From       │ Stage 1  │ Rebuild
# │ Stage 2  │ All     │ Stage 2  │ Cache      │ Stage 2  │ All
# │ Stage 3  │   ↓     │ Stage 3  │  (Skip)    │ Stage 3  │   ↓
# │ Stage 4  │ Save    │ Stage 4  │   │        │ Stage 4  │ Save
# ├──────────┤ Cache   ├──────────┤   ▼        ├──────────┤ Cache
# │ RasQberry│   ↓     │ RasQberry│ Build      │ RasQberry│   ↓
# └──────────┘         └──────────┘ Only       └──────────┘
# ============================================================================


# ============================================================================
# ENVIRONMENT VARIABLES REFERENCE
# ============================================================================
# Build configuration is split between workflow (CACHE_VERSION) and pi-gen-config.
# The workflow reads from pi-gen-config and applies branch-specific logic.
#
# Workflow Variables (env section below):
#    - CACHE_VERSION: GitHub Actions cache versioning
#
# Pi-gen-config Variables (centralized configuration file):
#
# 1. Build Quality Settings:
#    - COMPRESSION_LEVEL_DEV / COMPRESSION_LEVEL_PROD: Image compression (1 dev, 9 prod)
#    - COMPRESSION_LEVEL_DEV_FALLBACK: Higher level if dev image exceeds 2GB (6)
#    - SKIP_INITRAMFS / SKIP_INITRAMFS_PROD: Initramfs generation control
#
# 2. Console Configuration (read by workflow):
#    - CONSOLE_TYPE / CONSOLE_TYPE_PROD / CONSOLE_TYPE_DEV: Console output (hdmi/serial)
#    - BOOT_VERBOSITY / BOOT_VERBOSITY_PROD / BOOT_VERBOSITY_DEV: Boot output (splash/verbose)
#
# 3. RasQberry Configuration:
#    - RQB_REPO, RQB_GIT_USER, RQB_GIT_BRANCH: Repository settings (set by workflow)
#    - RQB_STD_VENV, RQB_PIGEN: Virtual environment settings
#
# 4. User & System Settings:
#    - FIRST_USER_NAME, IMG_NAME: User and image naming
#    - RELEASE, LOCALE_DEFAULT, TIMEZONE_DEFAULT: System configuration
#
# Data Flow: pi-gen-config → workflow reads/applies logic → pi-gen build
# ============================================================================

name: Rasqberry Pi Image Release v2

# Concurrency control: Only one workflow per branch, cancel previous runs
concurrency:
  group: rasqberry-build-${{ github.ref }}
  cancel-in-progress: true

# GitHub permissions required for this workflow
permissions:
  contents: write      # Create releases and push tags
  checks: write        # Update check status
  id-token: write      # Sign artifacts with cosign
  packages: write      # Push to GitHub container registry
  actions: write       # Trigger consolidation workflow

# ============================================================================
# WORKFLOW TRIGGERS
# ============================================================================
on:
  # Manual trigger with input parameters
  workflow_dispatch:
    inputs:
      version:
        description: 'Version number (required for main branch)'
        required: false
        type: string
      build_scope:
        description: 'What to build (default: dev=ab-only, main/beta=full)'
        required: false
        default: 'default'
        type: choice
        options:
          - default
          - ab-only
          - standard-image
          - full
          - no-release
      console_type:
        description: 'Console output (default: dev=serial, main/beta=hdmi)'
        required: false
        default: 'default'
        type: choice
        options:
          - default
          - hdmi
          - serial
      boot_verbosity:
        description: 'Boot verbosity (default: dev=verbose, main/beta=splash)'
        required: false
        default: 'default'
        type: choice
        options:
          - default
          - verbose
          - splash
      runner_type:
        description: 'Runner architecture (default=arm64 native, x64=QEMU emulated)'
        required: false
        default: 'default'
        type: choice
        options:
          - default
          - arm64
          - x64

  # Automatic trigger for development branches
  push:
    branches:
      - dev*    # Matches: dev, dev-feature, development, etc.

# ============================================================================
# GLOBAL ENVIRONMENT VARIABLES
# ============================================================================
env:
  # Cache versioning - increment this to invalidate all caches
  CACHE_VERSION: v2

# ============================================================================
# JOB 1: Version Management
# ============================================================================
# This job handles semantic versioning and creates git tags
# For main branch: Requires semantic version (e.g., 1.2.3)
# For dev branches: Auto-generates version with timestamp
jobs:
  rasqberry-push-version-number:
    name: "Rasqberry: Push version number"
    runs-on: ubuntu-latest
    outputs:
      # Version outputs for use in subsequent jobs
      version: ${{ steps.current-version.outputs.version }}
      version_num: ${{ steps.update-version.outputs.version_num }}
      commit_sha: ${{ steps.get-sha.outputs.commit_sha }}
      # Build scope (computed from input or branch defaults)
      build_scope: ${{ steps.compute-scope.outputs.build_scope }}
    
    steps:
      # Clone the repository with push permissions
      - name: "Rasqberry: Clone Repository"
        uses: actions/checkout@v4
        with:
          repository: ${{ github.repository }}
          token: ${{ secrets.GITHUB_TOKEN }}
          persist-credentials: true  # Needed for pushing tags

      # Generate timestamp for dev branch versions
      - name: Create Date
        id: create-date
        shell: bash
        run: |
          NOW="$(TZ=Europe/Berlin date +"%Y-%m-%d-%H%M%S")"
          echo "date=${NOW}" >> $GITHUB_OUTPUT

      # Read the current version from VERSION file
      - name: "Rasqberry: Get current version"
        id: current-version
        shell: bash
        run: |
          version=$(cat ./VERSION)
          echo "version=${version}" >> $GITHUB_OUTPUT

      # Update version file with new version number
      # Logic:
      # - Main branch: Must provide semantic version via input
      # - Dev branches: Use input if provided, else branch-timestamp
      - name: "Rasqberry: Add version file"
        id: update-version
        if: ${{ steps.current-version.outputs.version != github.event.inputs.version }}
        shell: bash
        env:
          VERSION_INPUT: ${{ github.event.inputs.version }}
          CREATE_DATE: ${{ steps.create-date.outputs.date }}
          BUILD_SCOPE_INPUT: ${{ github.event.inputs.build_scope }}
        run: |
          BRANCH_NAME=${GITHUB_REF#refs/heads/}

          # Main branch requires explicit semantic version for releases
          if [ "$BRANCH_NAME" = "main" ]; then
            if [ -z "$VERSION_INPUT" ]; then
              echo "Error: On 'main' branch you must supply a semantic version via workflow inputs." >&2
              exit 1
            fi
            VERSION_NUMBER="$VERSION_INPUT"
          else
            # Dev branches: Use input or generate from branch name + timestamp
            if [ -n "$VERSION_INPUT" ]; then
              VERSION_NUMBER="$VERSION_INPUT"
            else
              VERSION_NUMBER="${BRANCH_NAME}-${CREATE_DATE}"
            fi
          fi

          echo "version_num=$VERSION_NUMBER" >> $GITHUB_OUTPUT
          echo "$VERSION_NUMBER" > ./VERSION

      # Validate semantic version format for main branch releases
      # Skip for no-release builds (no version validation needed)
      - name: "Validate semantic version"
        if: github.ref == 'refs/heads/main' && github.event.inputs.build_scope != 'no-release'
        shell: bash
        env:
          VERSION_INPUT: ${{ github.event.inputs.version }}
        run: |
          echo "$VERSION_INPUT" | grep -Eq '^[0-9]+\.[0-9]+\.[0-9]+$' \
            || { echo "⛔ Invalid semantic version: $VERSION_INPUT" >&2; exit 1; }
        
      # Commit version change and create/update git tag
      # Tags: v1.2.3 for main, branch-timestamp for dev
      # Skip for no-release builds (no version bump needed)
      - name: "Rasqberry: git add & commit & push"
        id: version-commit
        if: github.event.inputs.build_scope != 'no-release'
        uses: EndBug/add-and-commit@v9
        with:
          add: "./VERSION"
          default_author: github_actions
          message: "Bump version to ${{ steps.update-version.outputs.version_num }}"
          github_token: ${{ secrets.GITHUB_TOKEN }}
          # Force push tags to allow re-running builds with same version
          tag: ${{ github.ref == 'refs/heads/main' && format('v{0} --force', steps.update-version.outputs.version_num) || format('{0} --force', steps.update-version.outputs.version_num) }}
          push: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      # Output the new commit SHA for downstream jobs to checkout
      - name: "Get commit SHA"
        id: get-sha
        run: |
          echo "commit_sha=$(git rev-parse HEAD)" >> $GITHUB_OUTPUT

      # Compute effective build scope from input or branch defaults
      - name: "Compute build scope"
        id: compute-scope
        run: |
          BRANCH_NAME=${GITHUB_REF#refs/heads/}
          INPUT_SCOPE="${{ github.event.inputs.build_scope }}"

          # Determine effective scope
          if [ "$INPUT_SCOPE" == "default" ] || [ -z "$INPUT_SCOPE" ]; then
            # Apply branch-based defaults
            if [[ "$BRANCH_NAME" == dev* ]]; then
              SCOPE="ab-only"
              echo "Using default scope for dev branch: ab-only (A/B image only)"
            else
              SCOPE="full"
              echo "Using default scope for $BRANCH_NAME branch: full (both images)"
            fi
          else
            SCOPE="$INPUT_SCOPE"
            echo "Using specified scope: $SCOPE"
          fi

          echo "build_scope=$SCOPE" >> $GITHUB_OUTPUT
          echo "Effective build scope: $SCOPE"

      # Install cosign for artifact signing (future enhancement)
      - name: Install cosign
        uses: sigstore/cosign-installer@v3.5.0
        with:
          cosign-release: 'v2.2.4'

  # ============================================================================
  # JOB 2: Create GitHub Release
  # ============================================================================
  # Creates a GitHub release with changelog for the new version
  # Skip for no-release builds (build runs without creating release)
  release:
    name: Create Release
    needs: rasqberry-push-version-number
    runs-on: ubuntu-latest
    if: needs.rasqberry-push-version-number.outputs.build_scope != 'no-release'
    outputs:
      # Release outputs for asset upload
      id: ${{ steps.create-release.outputs.id }}
      upload_url: ${{ steps.create-release.outputs.upload_url }}

    steps:
      # Checkout with full history for changelog generation
      # Use the commit with updated VERSION file from Job 1
      - name: Checkout
        uses: actions/checkout@v4
        with:
          ref: ${{ needs.rasqberry-push-version-number.outputs.commit_sha }}
          fetch-depth: 0  # Full history needed for git-cliff

      # Get the previous tag (skip the tag we just created) for changelog generation
      # Filter by branch name to avoid picking tags from other branches
      - name: Get previous tag
        id: latest_tag
        shell: bash
        run: |
          BRANCH_NAME="${GITHUB_REF_NAME}"

          # For main branch, look for v* tags; for dev branches, look for branch-* tags
          if [[ "$BRANCH_NAME" == "main" ]]; then
            PREV_TAG=$(git tag --sort=-creatordate | grep '^v' | sed -n '2p')
          else
            PREV_TAG=$(git tag --sort=-creatordate | grep "^${BRANCH_NAME}-" | sed -n '2p')
          fi

          echo "TAG_NAME=${PREV_TAG}" >> $GITHUB_OUTPUT
          echo "Found previous tag: ${PREV_TAG}"

      # Generate changelog from commits since last tag
      - name: Generate a changelog
        uses: orhun/git-cliff-action@v1
        id: changelog
        with:
          config: ./cliff-release.toml  # Changelog format configuration
          args: ${{ steps.latest_tag.outputs.TAG_NAME }}..HEAD

      # Create GitHub release with generated changelog
      # Release naming:
      # - Main: "rasqberry-v1.2.3"
      # - Dev: "rasqberry-dev-2024-01-01-123456"
      - name: Create Release
        id: create-release
        uses: softprops/action-gh-release@v2.4.2
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          name: ${{ github.ref == 'refs/heads/main' && format('rasqberry-v{0}', needs.rasqberry-push-version-number.outputs.version_num) || format('rasqberry-{0}', needs.rasqberry-push-version-number.outputs.version_num) }}
          tag_name: ${{ github.ref == 'refs/heads/main' && format('v{0}', needs.rasqberry-push-version-number.outputs.version_num) || needs.rasqberry-push-version-number.outputs.version_num }}
          body: ${{ steps.changelog.outputs.content }}
          # Dev and beta are pre-releases, main is stable
          prerelease: ${{ github.ref != 'refs/heads/main' }}
          # Only main is marked as GitHub's official "latest"
          make_latest: ${{ github.ref == 'refs/heads/main' }}

  # ============================================================================
  # JOB 3: Build Raspberry Pi Image
  # ============================================================================
  # This job builds the actual Raspberry Pi OS image using pi-gen
  # Includes sophisticated caching for development builds
  build:
    name: Build Image
    needs: [rasqberry-push-version-number, release]
    if: always() && needs.rasqberry-push-version-number.result == 'success'
    # Runner selection: arm64=native (default), x64=emulated (QEMU)
    # ARM64 native runners are free for public repos since Jan 2025
    runs-on: ${{ github.event.inputs.runner_type == 'x64' && 'ubuntu-latest' || 'ubuntu-24.04-arm' }}
    outputs:
      # Output paths for the built image
      asset_path: ${{ steps.set-asset.outputs.asset_path }}
      asset_name: ${{ steps.set-asset.outputs.asset_name }}
      # Pre-computed metadata (saves ~2min by avoiding re-download in JSON job)
      extract_sha256: ${{ steps.set-asset.outputs.extract_sha256 }}
      extract_size: ${{ steps.set-asset.outputs.extract_size }}
      compressed_size: ${{ steps.set-asset.outputs.compressed_size }}

    steps:
      # Checkout the commit with updated VERSION file from Job 1
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: ${{ needs.rasqberry-push-version-number.outputs.commit_sha }}

      # Brief configuration summary at start of build
      - name: Build configuration
        run: |
          BRANCH="${GITHUB_REF#refs/heads/}"

          # Determine stream type
          if [[ "$BRANCH" == "main" || "$BRANCH" == "beta" ]]; then
            STREAM="prod"
            CONSOLE=$(grep "^CONSOLE_TYPE_PROD=" pi-gen-config | cut -d'=' -f2)
            VERBOSITY=$(grep "^BOOT_VERBOSITY_PROD=" pi-gen-config | cut -d'=' -f2)
            COMPRESS=$(grep "^COMPRESSION_LEVEL_PROD=" pi-gen-config | cut -d'=' -f2)
          else
            STREAM="dev"
            CONSOLE=$(grep "^CONSOLE_TYPE_DEV=" pi-gen-config | cut -d'=' -f2)
            VERBOSITY=$(grep "^BOOT_VERBOSITY_DEV=" pi-gen-config | cut -d'=' -f2)
            COMPRESS=$(grep "^COMPRESSION_LEVEL_DEV=" pi-gen-config | cut -d'=' -f2)
          fi

          echo "========================================"
          echo "BUILD CONFIGURATION"
          echo "========================================"
          echo "Branch: $BRANCH"
          echo "Commit: ${{ needs.rasqberry-push-version-number.outputs.commit_sha }}"
          echo "Version: ${{ needs.rasqberry-push-version-number.outputs.version_num }}"
          echo ""
          echo "Runner: ${{ runner.os }} / ${{ runner.arch }}"
          echo "Trigger: ${{ github.event_name }}"
          echo "Build scope: ${{ needs.rasqberry-push-version-number.outputs.build_scope }}"
          echo ""
          echo "Stream: $STREAM"
          echo "Console: $CONSOLE"
          echo "Boot verbosity: $VERBOSITY"
          echo "Compression: xz -$COMPRESS"
          echo "Cache version: ${{ env.CACHE_VERSION }}"
          echo "========================================"
          echo ""

      # ========================================================================
      # DISK SPACE OPTIMIZATION
      # ========================================================================
      # GitHub Actions runners have limited disk space
      # ARM64: ~46GB free (fewer pre-installed tools)
      # x86:   ~14GB free (many pre-installed tools)
      - name: Free up disk space
        run: |
          ARCH=$(uname -m)
          echo "=== Initial disk space (arch: $ARCH) ==="
          df -h /

          get_available_gb() {
            df / | awk 'NR==2 {printf "%.1f", $4/1024/1024}'
          }

          before=$(get_available_gb)
          start=$(date +%s)

          # Architecture-specific cleanup
          # ARM64 runners have fewer pre-installed tools, so we skip non-existent items
          case "$ARCH" in
            aarch64|arm64)
              echo "ARM64: Optimized cleanup..."
              # Directories confirmed to exist on ARM64 runners:
              sudo rm -rf /opt/hostedtoolcache &   # ~500MB tool cache
              sudo rm -rf /usr/share/dotnet &      # ~2GB .NET SDK
              sudo rm -rf /usr/share/swift &       # ~1.5GB Swift toolchain
              sudo rm -rf /usr/local/julia* &      # ~500MB Julia
              sudo rm -rf /usr/share/miniconda &   # ~400MB Miniconda
              sudo rm -rf /usr/local/aws-cli &     # ~200MB AWS CLI
              sudo rm -rf /imagegeneration &       # Runner provisioning artifacts
              sudo rm -rf /opt/pipx &              # pipx installations
              # Skip: android, ghc, ghcup, boost (not installed on ARM64)
              # Skip: apt-get remove (packages not installed, wastes 14s)
              ;;
            *)
              echo "x86: Full cleanup..."
              # All these exist on x86 runners:
              sudo rm -rf /opt/hostedtoolcache &
              sudo rm -rf /usr/local/lib/android &
              sudo rm -rf /usr/share/dotnet &
              sudo rm -rf /opt/ghc &
              sudo rm -rf /usr/local/.ghcup &
              sudo rm -rf /usr/local/share/boost &
              sudo rm -rf /usr/share/swift &
              sudo rm -rf /usr/local/julia* &
              sudo rm -rf /usr/share/miniconda &
              sudo rm -rf /imagegeneration &
              # apt-get remove (useful on x86, many packages installed)
              (
                sudo apt-get remove -y '^aspnetcore-.*' '^dotnet-.*' 'php.*' '^mongodb-.*' '^mysql-.*' azure-cli google-chrome-stable firefox powershell mono-devel 2>/dev/null || true
                sudo apt-get autoremove -y 2>/dev/null || true
              ) &
              ;;
          esac

          wait

          end=$(date +%s)
          after=$(get_available_gb)
          freed=$(echo "scale=1; $after - $before" | bc)
          echo ""
          echo "=== Cleanup complete: ${freed}GB freed in $((end - start))s ==="
          df -h /

      # ========================================================================
      # MEMORY OPTIMIZATION
      # ========================================================================
      # Pi-gen is memory intensive, especially during compression
      # Create large swap file to prevent OOM errors
      - name: Maximize available memory
        run: |
          ARCH=$(uname -m)
          echo "=== Initial memory status (arch: $ARCH) ==="
          free -h

          # Stop unnecessary services to free RAM
          # ARM64 has fewer services installed, so we only stop what exists
          echo "Stopping unnecessary services..."
          case "$ARCH" in
            aarch64|arm64)
              # Only these services exist on ARM64 runners:
              sudo systemctl stop mysql snapd 2>/dev/null || true
              # Skip: postgresql, apache2, google-chrome, firefox, dotnet (not installed)
              ;;
            *)
              # x86 has more services installed:
              sudo systemctl stop mysql postgresql apache2 snapd google-chrome firefox dotnet 2>/dev/null || true
              ;;
          esac

          # Remove existing swap and create larger one
          sudo swapoff -a || true
          sudo rm -f /swapfile || true

          echo "Creating 10GB swap file..."
          sudo fallocate -l 10G /swapfile
          sudo chmod 600 /swapfile
          sudo mkswap /swapfile
          sudo swapon /swapfile
          
          echo "=== Final memory status ==="
          free -h
          echo "=== Swap status ==="
          sudo swapon --show

      # ========================================================================
      # CACHE CONFIGURATION
      # ========================================================================
      # Display what cache key we're looking for (debugging aid)
      - name: List available caches
        run: |
          echo "Current branch: ${GITHUB_REF#refs/heads/}"
          echo "Cache key pattern: stage4-v2-$(date +"%Y-%m")-bookworm-arm64"
          # Note: Cache version defined in pi-gen-config (CACHE_VERSION)

      # Determine caching strategy for this build
      # Rules:
      # - Main/beta branches: Delete existing caches, build fresh, then save new caches
      # - Dev branches: Restore and save caches (incremental builds)
      - name: Determine caching strategy
        id: cache-decision
        run: |
          BRANCH_NAME=${GITHUB_REF#refs/heads/}

          if [[ "$BRANCH_NAME" == "main" || "$BRANCH_NAME" == "beta" ]]; then
            echo "use_cache=true" >> $GITHUB_OUTPUT
            echo "delete_cache=true" >> $GITHUB_OUTPUT
            echo "Cache enabled with deletion for production branch: $BRANCH_NAME"
            echo "  - Will delete existing caches for this branch"
            echo "  - Will build fresh and populate new caches"
          else
            echo "use_cache=true" >> $GITHUB_OUTPUT
            echo "delete_cache=false" >> $GITHUB_OUTPUT
            echo "Cache enabled for development branch: $BRANCH_NAME"
          fi

      # Generate cache keys with time-based rotation (daily for wheel cache)
      - name: Generate cache date
        id: cache-date
        if: steps.cache-decision.outputs.use_cache == 'true'
        run: |
          # Use year-month-day for wheel cache (daily refresh)
          CACHE_DATE=$(date +"%Y-%m-%d")
          echo "cache_date=${CACHE_DATE}" >> $GITHUB_OUTPUT

      # ========================================================================
      # CACHE DELETION (main/beta only)
      # ========================================================================
      # Delete existing caches for this branch before building fresh
      # This ensures main/beta always get latest packages while populating cache
      # for dev branches to use. Each branch only deletes its own caches.
      - name: Delete branch caches (main/beta only)
        if: steps.cache-decision.outputs.delete_cache == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          BRANCH_NAME=${GITHUB_REF#refs/heads/}
          REPO="${{ github.repository }}"

          echo "=== Deleting caches for branch: $BRANCH_NAME ==="

          # List and delete wheel caches for this branch
          echo "Deleting wheel caches..."
          WHEEL_CACHES=$(gh api "repos/${REPO}/actions/caches?ref=refs/heads/${BRANCH_NAME}&key=wheels-arm64" --jq '.actions_caches[].id' 2>/dev/null || echo "")
          if [ -n "$WHEEL_CACHES" ]; then
            echo "$WHEEL_CACHES" | while read cache_id; do
              echo "  Deleting wheel cache ID: $cache_id"
              gh api -X DELETE "repos/${REPO}/actions/caches/${cache_id}" || true
            done
          else
            echo "  No wheel caches found for branch $BRANCH_NAME"
          fi

          # List and delete base-stages caches for this branch
          echo "Deleting base-stages caches..."
          BASE_CACHES=$(gh api "repos/${REPO}/actions/caches?ref=refs/heads/${BRANCH_NAME}&key=base-stages" --jq '.actions_caches[].id' 2>/dev/null || echo "")
          if [ -n "$BASE_CACHES" ]; then
            echo "$BASE_CACHES" | while read cache_id; do
              echo "  Deleting base-stages cache ID: $cache_id"
              gh api -X DELETE "repos/${REPO}/actions/caches/${cache_id}" || true
            done
          else
            echo "  No base-stages caches found for branch $BRANCH_NAME"
          fi

          echo "Cache deletion complete"

      # ========================================================================
      # CACHE RESTORATION - Base Stages Only
      # ========================================================================
      # Cache only the base stages (0-4) which are shared across all dev branches
      # Custom stage-RQB2 always builds fresh to ensure branch-specific changes
      # For main/beta: caches were just deleted, so restore will miss → fresh build
      - name: Restore Base Stages Cache (0-4)
        id: cache-base-stages
        if: steps.cache-decision.outputs.use_cache == 'true'
        uses: actions/cache/restore@v4
        with:
          path: base-stages-cache.tar.gz
          key: base-stages-${{ env.CACHE_VERSION }}
          restore-keys: |
            base-stages-${{ env.CACHE_VERSION }}-

      # Cache ARM64 wheel files for Qiskit installation
      # Stores actual .whl files for instant installation (no network queries)
      # Daily TTL ensures packages are refreshed regularly for security updates
      # Note: main/beta delete caches first, build fresh, then save new caches
      - name: Cache ARM64 wheels
        id: cache-wheels
        if: steps.cache-decision.outputs.use_cache == 'true'
        uses: actions/cache@v4
        with:
          path: ~/wheel-cache
          # No restore-keys: require exact match to prevent stale cache on requirements change
          key: wheels-arm64-${{ steps.cache-date.outputs.cache_date }}-${{ hashFiles('RQB2-config/qiskit-requirements.txt') }}

      # ========================================================================
      # PI-GEN DEPENDENCIES
      # ========================================================================
      # Install required packages for running pi-gen
      - name: Install pi-gen dependencies
        run: |
          sudo apt-get update

          # Detect architecture
          ARCH=$(uname -m)
          echo "Runner architecture: $ARCH"

          # Install pi-gen dependencies
          # Note: qemu-user-static is required even on ARM64 because pi-gen's
          # build.sh checks for it as a dependency (though it won't be used on ARM64)
          sudo apt-get install -y \
            coreutils \
            quilt \
            parted \
            qemu-user-static \
            debootstrap \
            zerofree \
            zip \
            dosfstools \
            libarchive-tools \
            libcap2-bin \
            grep \
            rsync \
            xz-utils \
            zstd \
            file \
            git \
            curl \
            bc \
            gpg \
            pigz \
            lz4 \
            arch-test

          if [ "$ARCH" = "aarch64" ]; then
            echo "ARM64 runner detected - qemu-user-static installed for pi-gen check but won't be used"
          else
            echo "x86_64 runner detected - qemu-user-static will be used for ARM emulation"
          fi

      # Clone official Raspberry Pi image generator
      - name: Clone pi-gen
        run: |
          git clone --depth 1 --branch bookworm-arm64 https://github.com/RPi-Distro/pi-gen
          cd pi-gen

      # Optimize initramfs generation for dev builds
      - name: Apply deferred initramfs optimization
        run: |
          # Always defer initramfs generation until the end of the build
          # This saves ~12 minutes by avoiding 6 redundant generations
          # SKIP_INITRAMFS=0: Generate once at end (fast)
          # SKIP_INITRAMFS=1: Never generate (even faster)

          echo "=== Applying deferred initramfs optimization ==="
          echo "Initramfs will be deferred during package installations"

          # Create early-stage hook to disable initramfs before any packages are installed
          mkdir -p pi-gen/stage0/00-disable-initramfs

          cat > pi-gen/stage0/00-disable-initramfs/00-run-chroot.sh <<'EOF'
          #!/bin/bash -e
          # Defer initramfs generation to speed up package installations
          # The real initramfs will be generated at the end (if SKIP_INITRAMFS=0)

          echo "Deferring initramfs generation for faster builds..."

          # Divert update-initramfs to a no-op script
          if command -v update-initramfs >/dev/null 2>&1; then
            dpkg-divert --add --rename --divert /usr/sbin/update-initramfs.real /usr/sbin/update-initramfs
            cat > /usr/sbin/update-initramfs <<'INNER_EOF'
          #!/bin/sh
          echo "Deferring update-initramfs (will generate at end of build)"
          exit 0
          INNER_EOF
            chmod +x /usr/sbin/update-initramfs
            echo "✓ update-initramfs diverted"
          fi

          # Divert mkinitramfs as well
          if command -v mkinitramfs >/dev/null 2>&1; then
            dpkg-divert --add --rename --divert /usr/sbin/mkinitramfs.real /usr/sbin/mkinitramfs
            cat > /usr/sbin/mkinitramfs <<'INNER_EOF'
          #!/bin/sh
          echo "Deferring mkinitramfs (will generate at end of build)"
          exit 0
          INNER_EOF
            chmod +x /usr/sbin/mkinitramfs
            echo "✓ mkinitramfs diverted"
          fi

          echo "Initramfs generation deferred successfully"
          EOF

          chmod +x pi-gen/stage0/00-disable-initramfs/00-run-chroot.sh
          echo "✓ Created deferred initramfs hook in stage0"

      # Extract cached base stages if cache was hit (after pi-gen exists)
      - name: Extract cached base stages
        if: steps.cache-decision.outputs.use_cache == 'true' && steps.cache-base-stages.outputs.cache-hit == 'true'
        run: |
          echo "Extracting cached base stages..."
          if [ -f "base-stages-cache.tar.gz" ]; then
            cd pi-gen
            sudo tar -xzf ../base-stages-cache.tar.gz
            echo "Cache extracted successfully"
            
            # Verify extraction
            if [ -d "work" ]; then
              echo "✓ Work directory restored from cache"
              ls -la work/
            else
              echo "❌ Work directory not found after extraction!"
              echo "Available directories:"
              ls -la
              exit 1
            fi
            
            # Clean up cache file to save space
            rm -f ../base-stages-cache.tar.gz
          else
            echo "❌ Cache file not found!"
            exit 1
          fi

      # ========================================================================
      # DYNAMIC CONFIGURATION
      # ========================================================================
      # Update build configuration with branch-specific settings and Git context
      # This ensures the image builds with correct quality and repository settings
      - name: Update pi-gen-config with branch-specific settings
        run: |
          BRANCH_NAME="${GITHUB_REF_NAME}"
          
          # Branch-specific quality settings override using config file values
          if [[ "$BRANCH_NAME" == "main" || "$BRANCH_NAME" == "beta" ]]; then
            echo "Applying production build settings for: $BRANCH_NAME"

            # Read production values from config file
            SKIP_INITRAMFS_PROD=$(grep "^SKIP_INITRAMFS_PROD=" pi-gen-config | cut -d'=' -f2)
            COMPRESSION_FORMAT_PROD=$(grep "^COMPRESSION_FORMAT_PROD=" pi-gen-config | cut -d'=' -f2)
            COMPRESSION_FORMAT_PROD="${COMPRESSION_FORMAT_PROD:-xz}"
            COMPRESSION_LEVEL_PROD=$(grep "^COMPRESSION_LEVEL_PROD=" pi-gen-config | cut -d'=' -f2)
            COMPRESSION_LEVEL_PROD="${COMPRESSION_LEVEL_PROD:-9}"

            # Apply production values
            sed -i "s/^SKIP_INITRAMFS=.*/SKIP_INITRAMFS=${SKIP_INITRAMFS_PROD}/" pi-gen-config
            echo "COMPRESSION_FORMAT=${COMPRESSION_FORMAT_PROD}" >> "$GITHUB_ENV"
            echo "COMPRESSION_LEVEL=${COMPRESSION_LEVEL_PROD}" >> "$GITHUB_ENV"
          else
            echo "Using development build settings for: $BRANCH_NAME"
            # Read dev compression format, level, and fallback level
            COMPRESSION_FORMAT_DEV=$(grep "^COMPRESSION_FORMAT_DEV=" pi-gen-config | cut -d'=' -f2)
            COMPRESSION_FORMAT_DEV="${COMPRESSION_FORMAT_DEV:-xz}"
            COMPRESSION_LEVEL_DEV=$(grep "^COMPRESSION_LEVEL_DEV=" pi-gen-config | cut -d'=' -f2)
            COMPRESSION_LEVEL_DEV="${COMPRESSION_LEVEL_DEV:-1}"
            COMPRESSION_LEVEL_DEV_FALLBACK=$(grep "^COMPRESSION_LEVEL_DEV_FALLBACK=" pi-gen-config | cut -d'=' -f2)
            COMPRESSION_LEVEL_DEV_FALLBACK="${COMPRESSION_LEVEL_DEV_FALLBACK:-6}"
            echo "COMPRESSION_FORMAT=${COMPRESSION_FORMAT_DEV}" >> "$GITHUB_ENV"
            echo "COMPRESSION_LEVEL=${COMPRESSION_LEVEL_DEV}" >> "$GITHUB_ENV"
            echo "COMPRESSION_LEVEL_FALLBACK=${COMPRESSION_LEVEL_DEV_FALLBACK}" >> "$GITHUB_ENV"
          fi

          # Disable pi-gen compression - we'll compress both images (standard + A/B) ourselves
          # This enables building A/B image directly without download/decompress cycle
          sed -i "s/^DEPLOY_COMPRESSION=.*/DEPLOY_COMPRESSION=none/" pi-gen-config
          echo "Pi-gen compression disabled - workflow will compress images after A/B conversion"
          
          # Conditional Git variable update (only if placeholder present)
          if grep -q "will-be-set-in-gh-workflow" pi-gen-config; then
            echo "Updating Git variables (placeholder values detected)"
            REPO_NAME="${GITHUB_REPOSITORY#*/}"
            REPO_OWNER="${GITHUB_REPOSITORY_OWNER}"

            sed -i "s|RQB_REPO=will-be-set-in-gh-workflow|RQB_REPO=${REPO_NAME}|" pi-gen-config
            sed -i "s|RQB_GIT_USER=will-be-set-in-gh-workflow|RQB_GIT_USER=${REPO_OWNER}|" pi-gen-config
            sed -i "s|RQB_GIT_BRANCH=will-be-set-in-gh-workflow|RQB_GIT_BRANCH=${BRANCH_NAME}|" pi-gen-config
          else
            echo "Git variables manually set, preserving existing values"
          fi

          # Update console configuration from workflow inputs
          # Read branch-specific defaults from pi-gen-config
          if [[ "$BRANCH_NAME" == "main" || "$BRANCH_NAME" == "beta" ]]; then
            CONSOLE_DEFAULT=$(grep "^CONSOLE_TYPE_PROD=" pi-gen-config | cut -d'=' -f2)
            BOOT_DEFAULT=$(grep "^BOOT_VERBOSITY_PROD=" pi-gen-config | cut -d'=' -f2)
          else
            CONSOLE_DEFAULT=$(grep "^CONSOLE_TYPE_DEV=" pi-gen-config | cut -d'=' -f2)
            BOOT_DEFAULT=$(grep "^BOOT_VERBOSITY_DEV=" pi-gen-config | cut -d'=' -f2)
          fi

          # Apply 'default' input: use branch-based defaults
          # For dev: serial/verbose, For beta/main: hdmi/splash
          CONSOLE_INPUT="${{ inputs.console_type }}"
          BOOT_INPUT="${{ inputs.boot_verbosity }}"
          if [ "$CONSOLE_INPUT" == "default" ] || [ -z "$CONSOLE_INPUT" ]; then
            CONSOLE_TYPE="$CONSOLE_DEFAULT"
          else
            CONSOLE_TYPE="$CONSOLE_INPUT"
          fi
          if [ "$BOOT_INPUT" == "default" ] || [ -z "$BOOT_INPUT" ]; then
            BOOT_VERBOSITY="$BOOT_DEFAULT"
          else
            BOOT_VERBOSITY="$BOOT_INPUT"
          fi

          sed -i "s|^CONSOLE_TYPE=.*|CONSOLE_TYPE=${CONSOLE_TYPE}|" pi-gen-config
          sed -i "s|^BOOT_VERBOSITY=.*|BOOT_VERBOSITY=${BOOT_VERBOSITY}|" pi-gen-config
          echo "Console configuration: CONSOLE_TYPE=${CONSOLE_TYPE}, BOOT_VERBOSITY=${BOOT_VERBOSITY}"

          # Set custom IMG_FILENAME based on VERSION file
          # This overrides pi-gen's default naming (IMG_DATE-IMG_NAME)
          # Naming convention:
          #   Main: rasqberry-v{version} (e.g., rasqberry-v1.2.3)
          #   Beta: rasqberry-beta-{date} (e.g., rasqberry-beta-2025-10-25)
          #   Dev: rasqberry-{branch}-{date}-{time} (e.g., rasqberry-dev-review01-2025-10-25-112716)

          VERSION=$(cat VERSION)
          echo "VERSION file content: $VERSION"

          if [[ "$VERSION" =~ ^v[0-9]+\.[0-9]+\.[0-9]+ ]]; then
            # Semantic version (main branch)
            IMG_FILENAME="rasqberry-${VERSION}"
          elif [[ "$VERSION" =~ ^beta-([0-9]{4}-[0-9]{2}-[0-9]{2})-[0-9]{6}$ ]]; then
            # Beta branch - date only (no time)
            BUILD_DATE="${BASH_REMATCH[1]}"
            IMG_FILENAME="rasqberry-beta-${BUILD_DATE}"
          elif [[ "$VERSION" =~ ^(.+)-([0-9]{4}-[0-9]{2}-[0-9]{2})-([0-9]{6})$ ]]; then
            # Dev branch - full timestamp
            BRANCH_NAME_FROM_VERSION="${BASH_REMATCH[1]}"
            BUILD_DATE="${BASH_REMATCH[2]}"
            BUILD_TIME="${BASH_REMATCH[3]}"
            IMG_FILENAME="rasqberry-${BRANCH_NAME_FROM_VERSION}-${BUILD_DATE}-${BUILD_TIME}"
          else
            # Fallback
            IMG_FILENAME="rasqberry-$(date +%Y-%m-%d)"
          fi

          # Debug: Show current directory and file status
          echo "DEBUG: Current directory: $(pwd)"
          echo "DEBUG: pi-gen-config exists: $(test -f pi-gen-config && echo 'YES' || echo 'NO')"
          echo "DEBUG: pi-gen-config size before append: $(wc -l < pi-gen-config 2>/dev/null || echo 'FILE NOT FOUND')"

          echo "IMG_FILENAME=${IMG_FILENAME}" >> pi-gen-config
          echo "Configured image filename: ${IMG_FILENAME}.img"

          # Debug: Verify the append worked
          echo "DEBUG: pi-gen-config size after append: $(wc -l < pi-gen-config)"
          echo "DEBUG: Last 3 lines of pi-gen-config:"
          tail -3 pi-gen-config

          # Show final configuration for verification
          echo "=== Final pi-gen-config settings ==="
          echo "Build Quality:"
          grep -E "^(SKIP_INITRAMFS|DEPLOY_COMPRESSION)=" pi-gen-config
          echo "Image Filename:"
          grep "^IMG_FILENAME=" pi-gen-config
          echo "Git Configuration:"
          grep -E "^RQB_(REPO|GIT_USER|GIT_BRANCH)=" pi-gen-config
          echo "Console Configuration:"
          grep -E "^(CONSOLE_TYPE|BOOT_VERBOSITY)=" pi-gen-config

      # Configure pi-gen with our custom settings
      - name: Configure build
        run: |
          cd pi-gen
          cp ../pi-gen-config config

      # ========================================================================
      # BASE STAGE BUILD (Conditional)
      # ========================================================================
      # Build stages 0-4 only if:
      # 1. Cache is disabled, OR
      # 2. Cache miss (no existing cache found)
      - name: Build base stages (if not cached or cache disabled)
        if: steps.cache-decision.outputs.use_cache == 'false' || steps.cache-base-stages.outputs.cache-hit != 'true'
        run: |
          cd pi-gen
          echo "Building base stages 0-4..."
          
          # Configure to build only base stages
          echo 'STAGE_LIST="stage0 stage1 stage2 stage3 stage4"' >> config
          
          # Skip image export for intermediate stages (saves time)
          touch ./stage0/SKIP_IMAGES ./stage1/SKIP_IMAGES ./stage2/SKIP_IMAGES
          touch ./stage3/SKIP_IMAGES ./stage4/SKIP_IMAGES
          
          # Skip stage5 (desktop software - not needed)
          touch ./stage5/SKIP
          
          # Run the build
          sudo ./build.sh
          
          # Clean up any custom stage remnants to prepare for fresh custom build
          sudo rm -rf work/*/stage-RQB2 || true
          
          echo "Base stages 0-4 build completed"

      # ========================================================================
      # IMMEDIATE CACHE SAVE - Base Stages
      # ========================================================================
      # Save the cache immediately after base stages complete to prevent loss
      # if custom stage fails. Only save if we built the base stages.
      - name: Save Base Stages Cache (Immediate)
        if: steps.cache-decision.outputs.use_cache == 'true' && (steps.cache-base-stages.outputs.cache-hit != 'true')
        run: |
          echo "Creating tarball of base stages work directory..."
          cd pi-gen
          
          # Create tarball with fast compression, excluding stage-RQB2 directories
          sudo tar -czf ../base-stages-cache.tar.gz \
            --exclude="work/*/stage-RQB2" \
            work/
          
          echo "Tarball created successfully"
          ls -lh ../base-stages-cache.tar.gz
          
      # Cache the tarball (separate step for cleaner error handling)
      - name: Cache Base Stages Tarball
        if: steps.cache-decision.outputs.use_cache == 'true' && (steps.cache-base-stages.outputs.cache-hit != 'true')
        uses: actions/cache/save@v4
        with:
          path: base-stages-cache.tar.gz
          key: base-stages-${{ env.CACHE_VERSION }}

      # Show system resources before main build
      - name: show swap and memory status before build
        run: |
          echo "=== Current memory status ==="
          free -h
          swapon --show

      # ========================================================================
      # CUSTOM STAGE BUILD
      # ========================================================================
      # Copy our custom RasQberry stage into pi-gen (after base stages are built)
      - name: Copy custom stage
        run: cp -r stage-RQB2 pi-gen/

      # Build the RasQberry-specific stage on top of base image
      - name: Build custom stage
        run: |
          cd pi-gen
          
          # Check if we have cached base stages
          if [ "${{ steps.cache-decision.outputs.use_cache }}" == "true" ] && [ "${{ steps.cache-base-stages.outputs.cache-hit }}" == "true" ]; then
            echo "Using cached base stages..."
            
            # Find the cached work directory
            WORK_DIR=$(find work -maxdepth 1 -type d ! -name work | head -1)
            
            if [ -n "$WORK_DIR" ]; then
              echo "Found cached work directory: $WORK_DIR"
              
              # Verify stage4 rootfs exists in cache
              if [ ! -d "$WORK_DIR/stage4/rootfs" ]; then
                echo "ERROR: stage4/rootfs missing from cache!"
                echo "Cache appears incomplete. Clear the cache and rebuild."
                exit 1
              fi
              
              # Set up for continuation build
              WORK_NAME=$(basename "$WORK_DIR")
              WORK_DIR_ABS="$(pwd)/work/$WORK_NAME"
              
              # Configure for continuation build from cached base
              echo "CONTINUE=1" >> config
              echo "WORK_DIR=$WORK_DIR_ABS" >> config
              echo 'STAGE_LIST="./stage-RQB2"' >> config
              
              # Ensure stage4 is marked as complete for continuation
              sudo mkdir -p "$WORK_DIR_ABS/stage4"
              sudo touch "$WORK_DIR_ABS/stage4/.build_done"
              
              # Update package lists in cached rootfs (may be stale)
              echo "Refreshing package lists in cached rootfs..."
              sudo chroot "$WORK_DIR_ABS/stage4/rootfs" apt-get update || echo "Warning: Failed to update package lists"

              # Note: Pip cache is now managed by stage-RQB2 scripts (00-run.sh, 01-run.sh, 02-run.sh)
              # Stage scripts handle: restore cache → qiskit install → save cache → clean from image

            else
              echo "ERROR: No work directory found in cache!"
              exit 1
            fi
          else
            echo "Building custom stage on fresh base..."

            # For fresh builds or no cache, continue from base build
            echo "CONTINUE=1" >> config
            echo 'STAGE_LIST="./stage-RQB2"' >> config
          fi
          
          # Show final configuration for debugging
          echo "=== Final config file ==="
          cat config
          echo "========================"

          # Prepare wheel cache for build (copy to sudo-accessible location)
          # Wheels provide instant installation without downloads
          echo "=== Preparing wheel cache for build ==="
          if [ -d ~/wheel-cache ] && [ -n "$(ls -A ~/wheel-cache 2>/dev/null)" ]; then
            cp -r ~/wheel-cache ../wheel-cache-host
            WHEEL_COUNT=$(find ../wheel-cache-host -name "*.whl" 2>/dev/null | wc -l || echo "0")
            echo "Copied wheel cache to build directory: $(du -sh ../wheel-cache-host | cut -f1) ($WHEEL_COUNT wheels)"
          else
            echo "No wheel cache available (first build or cache miss)"
            mkdir -p ../wheel-cache-host
          fi

          # Build the custom RasQberry stage
          sudo ./build.sh

          # Copy updated wheel cache back to GitHub Actions location
          echo "=== Saving updated wheel cache ==="
          if [ -d ../wheel-cache-host ] && [ -n "$(ls -A ../wheel-cache-host 2>/dev/null)" ]; then
            rm -rf ~/wheel-cache
            cp -r ../wheel-cache-host ~/wheel-cache
            WHEEL_COUNT=$(find ~/wheel-cache -name "*.whl" 2>/dev/null | wc -l || echo "0")
            CACHE_SIZE=$(du -sh ~/wheel-cache 2>/dev/null | cut -f1 || echo "0")
            echo "Updated wheel cache: $CACHE_SIZE ($WHEEL_COUNT wheels)"
          else
            echo "No wheel cache to save"
          fi

      # Note: Caches are managed by workflow + stage scripts:
      #   Workflow: Copies ~/wheel-cache to host dir (before build)
      #   00-run.sh: Restores wheel cache to /tmp/wheels in rootfs
      #   00-run-chroot.sh: Installs qiskit using --find-links=/tmp/wheels
      #   01-run.sh: Saves new wheels back to host
      #   02-run.sh: Cleans caches from rootfs (reduces image size)
      #   Workflow: Copies host cache back to ~/ (after build)
      # actions/cache@v4 persists wheel cache to GitHub Actions cache

      # ========================================================================
      # IMAGE FINALIZATION AND A/B CONVERSION
      # ========================================================================
      # Find uncompressed image, create A/B variant, compress both, upload both
      # This avoids the download/decompress cycle of a separate A/B job
      - name: Create A/B variant and compress images
        id: set-asset
        run: |
          # Ensure we're in the right directory
          if [ ! -d "pi-gen" ]; then
            echo "Error: pi-gen directory not found in $(pwd)"
            ls -la
            exit 1
          fi
          mkdir -p deploy

          # Find the uncompressed image file (DEPLOY_COMPRESSION=none)
          RAW_IMAGE=$(find pi-gen/deploy -name "*.img" -type f | head -n 1)

          if [ -z "$RAW_IMAGE" ]; then
            echo "Error: No uncompressed image found in pi-gen/deploy"
            ls -la pi-gen/deploy/ || echo "deploy directory does not exist"
            exit 1
          fi

          echo "Pi-gen created: $(basename "$RAW_IMAGE")"
          echo "Size: $(du -h "$RAW_IMAGE" | cut -f1)"

          # Determine compression settings from environment (all builds use xz)
          COMPRESS_LEVEL="${COMPRESSION_LEVEL:-1}"
          COMPRESS_LEVEL_FALLBACK="${COMPRESSION_LEVEL_FALLBACK:-}"
          COMPRESS_EXT="xz"
          CONTENT_TYPE="application/x-xz"
          echo "Compression: xz level ${COMPRESS_LEVEL}"
          if [ -n "$COMPRESS_LEVEL_FALLBACK" ]; then
            echo "Fallback level: ${COMPRESS_LEVEL_FALLBACK} (if image exceeds 2GB)"
          fi

          # Determine correct base filename from VERSION file
          VERSION=$(cat VERSION)
          echo "VERSION file content: $VERSION"

          if [[ "$VERSION" =~ ^v[0-9]+\.[0-9]+\.[0-9]+ ]]; then
            BASE_FILENAME="rasqberry-${VERSION}"
          elif [[ "$VERSION" =~ ^beta-([0-9]{4}-[0-9]{2}-[0-9]{2})-[0-9]{6}$ ]]; then
            BUILD_DATE="${BASH_REMATCH[1]}"
            BASE_FILENAME="rasqberry-beta-${BUILD_DATE}"
          elif [[ "$VERSION" =~ ^(.+)-([0-9]{4}-[0-9]{2}-[0-9]{2})-([0-9]{6})$ ]]; then
            BRANCH_NAME_FROM_VERSION="${BASH_REMATCH[1]}"
            BUILD_DATE="${BASH_REMATCH[2]}"
            BUILD_TIME="${BASH_REMATCH[3]}"
            BASE_FILENAME="rasqberry-${BRANCH_NAME_FROM_VERSION}-${BUILD_DATE}-${BUILD_TIME}"
          else
            BASE_FILENAME=$(basename "$RAW_IMAGE" .img)
            echo "Warning: VERSION format not recognized, using pi-gen filename"
          fi

          STANDARD_IMG="deploy/${BASE_FILENAME}.img"
          AB_IMG="deploy/${BASE_FILENAME}-ab.img"

          echo "Standard image: $STANDARD_IMG"
          echo "A/B image: $AB_IMG"

          # Show initial disk space
          echo ""
          echo "=== Disk Space Before Processing ==="
          df -h /

          # MOVE raw image to deploy with correct name (saves 8GB vs copy)
          # Note: pi-gen creates files as root, so we need sudo
          sudo mv "$RAW_IMAGE" "$STANDARD_IMG"
          echo "Moved raw image to: $STANDARD_IMG"

          # Clean up pi-gen directories to free space for A/B conversion
          sudo rm -rf pi-gen/deploy pi-gen/work
          echo "Cleaned up pi-gen/deploy and pi-gen/work"
          echo ""
          echo "=== Disk Space After Cleanup ==="
          df -h /

          # ================================================================
          # CREATE A/B BOOT VARIANT (for ab-only and full builds)
          # ================================================================
          BUILD_SCOPE="${{ needs.rasqberry-push-version-number.outputs.build_scope }}"
          if [[ "$BUILD_SCOPE" == "full" ]] || [[ "$BUILD_SCOPE" == "ab-only" ]]; then
            echo ""
            echo "=== Creating A/B Boot Variant ==="

            # Read console configuration from pi-gen-config (already set in earlier step)
            CONSOLE_TYPE=$(grep "^CONSOLE_TYPE=" pi-gen-config | cut -d'=' -f2)
            BOOT_VERBOSITY=$(grep "^BOOT_VERBOSITY=" pi-gen-config | cut -d'=' -f2)

            echo "Console: ${CONSOLE_TYPE}, Boot: ${BOOT_VERBOSITY}"

            # Run conversion script
            sudo CONSOLE_TYPE="${CONSOLE_TYPE}" BOOT_VERBOSITY="${BOOT_VERBOSITY}" \
              ./stage-RQB2/08-ab-boot-support/files/convert-to-ab-boot-v3.sh \
              "$STANDARD_IMG" \
              "$AB_IMG"

            if [ ! -f "$AB_IMG" ]; then
              echo "Error: A/B conversion failed"
              exit 1
            fi
            echo "A/B image created: $(du -h "$AB_IMG" | cut -f1)"

            # Verify A/B image configuration
            echo ""
            echo "=== Verifying A/B Boot Configuration ==="

            # Show partition layout
            echo "Partition layout:"
            sudo parted "$AB_IMG" print

            # Mount and verify configuration files
            AB_LOOP=$(sudo losetup -fP --show "$AB_IMG")
            TEMP_MOUNT=$(mktemp -d)

            # Check cmdline.txt in bootfs-a
            sudo mount "${AB_LOOP}p2" "$TEMP_MOUNT"
            echo ""
            echo "cmdline.txt (bootfs-a):"
            cat "$TEMP_MOUNT/cmdline.txt"
            sudo umount "$TEMP_MOUNT"

            # Check fstab in rootfs-a
            sudo mount "${AB_LOOP}p5" "$TEMP_MOUNT"
            echo ""
            echo "/etc/fstab (rootfs-a):"
            cat "$TEMP_MOUNT/etc/fstab"
            sudo umount "$TEMP_MOUNT"

            # Check config partition
            sudo mount "${AB_LOOP}p1" "$TEMP_MOUNT"
            echo ""
            echo "autoboot.txt:"
            cat "$TEMP_MOUNT/autoboot.txt"
            sudo umount "$TEMP_MOUNT"

            # Cleanup
            rmdir "$TEMP_MOUNT"
            sudo losetup -d "$AB_LOOP"
            echo ""
            echo "=== A/B Configuration Verified ==="
          fi

          # ================================================================
          # CALCULATE METADATA AND COMPRESS IMAGES
          # ================================================================
          # Calculate SHA256 of uncompressed image BEFORE compression
          # This saves ~2 minutes by avoiding re-download in JSON job
          echo ""
          echo "=== Calculating image metadata before compression ==="

          # Calculate uncompressed size and SHA256 for standard image
          echo "Calculating SHA256 of $(basename "$STANDARD_IMG")..."
          EXTRACT_SIZE=$(stat -c%s "$STANDARD_IMG")
          EXTRACT_SHA256=$(sha256sum "$STANDARD_IMG" | cut -d' ' -f1)
          echo "  Uncompressed size: $((EXTRACT_SIZE / 1024 / 1024)) MB"
          echo "  SHA256: $EXTRACT_SHA256"

          echo ""
          echo "=== Compressing Images with xz -${COMPRESS_LEVEL} ==="

          # Define 2GB limit in bytes (2 * 1024 * 1024 * 1024)
          MAX_SIZE_BYTES=2147483648

          # Helper to run xz and handle exit code 2 (permission warning)
          # xz returns 2 when compression succeeds but it can't set file group
          run_xz() {
            local XZ_EXIT
            xz "$@" || XZ_EXIT=$?
            if [ "${XZ_EXIT:-0}" -eq 2 ]; then
              echo "  (ignoring xz exit code 2 - permission warning)"
              return 0
            fi
            return ${XZ_EXIT:-0}
          }

          # Function to compress with fallback if exceeds 2GB
          compress_with_fallback() {
            local IMG="$1"
            local LEVEL="$2"
            local FALLBACK="$3"
            local COMPRESSED="${IMG}.xz"

            echo "Compressing $(basename "$IMG") with xz -${LEVEL}..."
            run_xz -${LEVEL} -T0 -k "$IMG"  # -k keeps original for potential recompression

            # Check size
            local SIZE=$(stat -c%s "$COMPRESSED")
            local SIZE_MB=$((SIZE / 1024 / 1024))
            echo "  Compressed size: ${SIZE_MB} MB"

            if [ "$SIZE" -gt "$MAX_SIZE_BYTES" ] && [ -n "$FALLBACK" ]; then
              echo "  ⚠️ Image exceeds 2GB limit (${SIZE_MB} MB)"
              echo "  Recompressing with xz -${FALLBACK}..."
              rm -f "$COMPRESSED"
              run_xz -${FALLBACK} -T0 -k "$IMG"
              SIZE=$(stat -c%s "$COMPRESSED")
              SIZE_MB=$((SIZE / 1024 / 1024))
              echo "  New size: ${SIZE_MB} MB"

              if [ "$SIZE" -gt "$MAX_SIZE_BYTES" ]; then
                echo "  ⚠️ WARNING: Image still exceeds 2GB after fallback compression!"
              else
                echo "  ✓ Image now under 2GB limit"
              fi
            elif [ "$SIZE" -gt "$MAX_SIZE_BYTES" ]; then
              echo "  ⚠️ WARNING: Image exceeds 2GB limit (no fallback configured)"
            else
              echo "  ✓ Image under 2GB limit"
            fi

            # Remove original uncompressed image
            rm -f "$IMG"
          }

          # Set output filenames
          STANDARD_COMPRESSED="deploy/${BASE_FILENAME}.img.${COMPRESS_EXT}"
          AB_COMPRESSED="deploy/${BASE_FILENAME}-ab.img.${COMPRESS_EXT}"

          # Compress images based on build scope
          # ab-only: Only compress AB image (delete standard to save space/time)
          # full: Compress both images
          # standard-image: Compress only standard image
          if [[ "$BUILD_SCOPE" == "ab-only" ]]; then
            echo "Build scope: ab-only - compressing A/B image only"
            # Delete standard image (not needed for ab-only)
            rm -f "$STANDARD_IMG"
            # Compress A/B image
            compress_with_fallback "$AB_IMG" "$COMPRESS_LEVEL" "$COMPRESS_LEVEL_FALLBACK"
          else
            # Compress standard image (for full and standard-image scopes)
            compress_with_fallback "$STANDARD_IMG" "$COMPRESS_LEVEL" "$COMPRESS_LEVEL_FALLBACK"
            # Compress A/B image if it exists (for full scope)
            if [ -f "$AB_IMG" ]; then
              compress_with_fallback "$AB_IMG" "$COMPRESS_LEVEL" "$COMPRESS_LEVEL_FALLBACK"
            fi
          fi

          echo ""
          echo "=== Compression Complete ==="
          if [ -f "$STANDARD_COMPRESSED" ]; then
            echo "Standard: $STANDARD_COMPRESSED ($(du -h "$STANDARD_COMPRESSED" | cut -f1))"
          fi
          if [ -f "$AB_COMPRESSED" ]; then
            echo "A/B: $AB_COMPRESSED ($(du -h "$AB_COMPRESSED" | cut -f1))"
          fi

          # Export outputs for standard image (if exists)
          if [ -f "$STANDARD_COMPRESSED" ]; then
            COMPRESSED_SIZE=$(stat -c%s "$STANDARD_COMPRESSED")
            echo "asset_path=$STANDARD_COMPRESSED" >> $GITHUB_OUTPUT
            echo "asset_name=$(basename $STANDARD_COMPRESSED)" >> $GITHUB_OUTPUT
            echo "compressed_size=$COMPRESSED_SIZE" >> $GITHUB_OUTPUT
          fi

          # Export pre-computed metadata (used by JSON job)
          echo "extract_sha256=$EXTRACT_SHA256" >> $GITHUB_OUTPUT
          echo "extract_size=$EXTRACT_SIZE" >> $GITHUB_OUTPUT

          # Export A/B image outputs (if exists)
          if [ -f "$AB_COMPRESSED" ]; then
            AB_COMPRESSED_SIZE=$(stat -c%s "$AB_COMPRESSED")
            echo "ab_asset_path=$AB_COMPRESSED" >> $GITHUB_OUTPUT
            echo "ab_asset_name=$(basename $AB_COMPRESSED)" >> $GITHUB_OUTPUT
            echo "ab_compressed_size=$AB_COMPRESSED_SIZE" >> $GITHUB_OUTPUT
            # For ab-only builds, use AB image size as primary compressed_size
            if [[ "$BUILD_SCOPE" == "ab-only" ]]; then
              echo "compressed_size=$AB_COMPRESSED_SIZE" >> $GITHUB_OUTPUT
            fi
          fi

          # Always set content_type (needed by both upload steps)
          # For ab-only builds, this is the only place it gets set
          echo "content_type=$CONTENT_TYPE" >> $GITHUB_OUTPUT

          # Log cache usage summary
          if [ "${{ steps.cache-decision.outputs.use_cache }}" == "true" ]; then
            echo ""
            echo "Build completed with base stage caching enabled"
            if [ "${{ steps.cache-base-stages.outputs.cache-hit }}" == "true" ]; then
              echo "Base stages cache hit - only custom stage-RQB2 was rebuilt"
            else
              echo "Base stages cache miss - full build was performed"
            fi
          else
            echo "Build completed without caching (production build)"
          fi

      # ========================================================================
      # RELEASE ASSET UPLOADS
      # ========================================================================
      # Upload images to the GitHub release based on build scope
      # ab-only: Only A/B image
      # full: Both standard and A/B images
      # standard-image: Only standard image
      # no-release: Skip uploads (no release created)
      - name: Upload Standard Image
        if: needs.rasqberry-push-version-number.outputs.build_scope != 'no-release' && needs.rasqberry-push-version-number.outputs.build_scope != 'ab-only' && steps.set-asset.outputs.asset_path != ''
        uses: actions/upload-release-asset@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          upload_url: ${{ needs.release.outputs.upload_url }}
          asset_path: ${{ steps.set-asset.outputs.asset_path }}
          asset_name: ${{ steps.set-asset.outputs.asset_name }}
          asset_content_type: ${{ steps.set-asset.outputs.content_type }}

      - name: Upload A/B Image
        if: needs.rasqberry-push-version-number.outputs.build_scope != 'no-release' && (needs.rasqberry-push-version-number.outputs.build_scope == 'full' || needs.rasqberry-push-version-number.outputs.build_scope == 'ab-only') && steps.set-asset.outputs.ab_asset_path != ''
        uses: actions/upload-release-asset@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          upload_url: ${{ needs.release.outputs.upload_url }}
          asset_path: ${{ steps.set-asset.outputs.ab_asset_path }}
          asset_name: ${{ steps.set-asset.outputs.ab_asset_name }}
          asset_content_type: ${{ steps.set-asset.outputs.content_type }}

      # ========================================================================
      # BUILD SUMMARY
      # ========================================================================
      - name: Build summary
        if: always()
        run: |
          echo ""
          echo "========================================"
          echo "BUILD SUMMARY"
          echo "========================================"
          echo ""
          echo "Build scope: ${{ needs.rasqberry-push-version-number.outputs.build_scope }}"
          echo "Runner: ${{ runner.os }} / ${{ runner.arch }}"
          echo ""

          # Check cache status
          echo "=== Cache Status ==="

          # Base stages cache
          if [ "${{ steps.cache-base-stages.outputs.cache-hit }}" == "true" ]; then
            echo "Base stages cache: HIT"
          else
            echo "Base stages cache: MISS (built fresh)"
          fi

          # Wheel cache
          if [ "${{ steps.cache-wheels.outputs.cache-hit }}" == "true" ]; then
            echo "Wheel cache: HIT"
            WHEEL_CACHE_HIT=true
          else
            echo "Wheel cache: MISS (downloaded during build)"
            WHEEL_CACHE_HIT=false
          fi

          echo ""

          # Provide guidance if wheel cache missed
          if [ "$WHEEL_CACHE_HIT" != "true" ] && [ "${{ steps.cache-decision.outputs.use_cache }}" == "true" ]; then
            echo "========================================"
            echo "TIP: Wheel cache miss detected"
            echo "========================================"
            echo ""
            echo "First build on this branch will populate the wheel cache."
            echo "Subsequent builds on dev branches will reuse the cache."
            echo ""
          fi

  # ============================================================================
  # JOB 4: Generate and Upload RQB-images.json
  # ============================================================================
  # Always generates the JSON file, optionally uploads to gh-pages
  # Skip for no-release builds (no release to document)
  update-json:
    name: Generate RQB-images.json
    needs: [rasqberry-push-version-number, release, build]
    runs-on: ubuntu-latest
    if: needs.rasqberry-push-version-number.outputs.build_scope != 'no-release'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          
      - name: Get image metadata from build job
        id: metadata
        run: |
          # Use pre-computed metadata from build job (saves ~2 minutes)
          # Previously this step downloaded 1.9GB and streamed decompression

          ASSET_NAME="${{ needs.build.outputs.asset_name }}"
          DOWNLOAD_URL="https://github.com/${{ github.repository }}/releases/download/${{ github.ref == 'refs/heads/main' && format('v{0}', needs.rasqberry-push-version-number.outputs.version_num) || needs.rasqberry-push-version-number.outputs.version_num }}/${ASSET_NAME}"

          # Get pre-computed values from build job
          EXTRACT_SHA256="${{ needs.build.outputs.extract_sha256 }}"
          EXTRACT_SIZE="${{ needs.build.outputs.extract_size }}"
          COMPRESSED_SIZE="${{ needs.build.outputs.compressed_size }}"

          # Include time for dev branches, date only for main/beta releases
          BRANCH_NAME="${GITHUB_REF#refs/heads/}"
          if [[ "$BRANCH_NAME" == "main" || "$BRANCH_NAME" == "beta" ]]; then
            RELEASE_DATE=$(date +"%Y-%m-%d")
          else
            RELEASE_DATE=$(date +"%Y-%m-%d %H:%M")
          fi

          echo "Using pre-computed metadata from build job (no download needed):"
          echo "  Compressed size: $COMPRESSED_SIZE bytes ($(numfmt --to=iec-i --suffix=B $COMPRESSED_SIZE))"
          echo "  Uncompressed size: $EXTRACT_SIZE bytes ($(numfmt --to=iec-i --suffix=B $EXTRACT_SIZE))"
          echo "  Uncompressed SHA256: $EXTRACT_SHA256"

          # Output all metadata
          echo "download_url=${DOWNLOAD_URL}" >> $GITHUB_OUTPUT
          echo "compressed_size=${COMPRESSED_SIZE}" >> $GITHUB_OUTPUT
          echo "extract_size=${EXTRACT_SIZE}" >> $GITHUB_OUTPUT
          echo "extract_sha256=${EXTRACT_SHA256}" >> $GITHUB_OUTPUT
          echo "release_date=${RELEASE_DATE}" >> $GITHUB_OUTPUT
          echo "version_num=${{ needs.rasqberry-push-version-number.outputs.version_num }}" >> $GITHUB_OUTPUT
          
      - name: Determine version type and stream
        id: version-type
        run: |
          BRANCH_NAME="${GITHUB_REF#refs/heads/}"
          VERSION_NUM="${{ steps.metadata.outputs.version_num }}"

          if [[ "$BRANCH_NAME" == "main" ]]; then
            echo "type=main" >> $GITHUB_OUTPUT
            echo "stream=stable" >> $GITHUB_OUTPUT
            echo "name=RasQberry Two (64-bit)" >> $GITHUB_OUTPUT
            echo "description=RasQberry stable release for exploring Quantum Computing and Qiskit" >> $GITHUB_OUTPUT
            echo "tag=v${VERSION_NUM}" >> $GITHUB_OUTPUT
            echo "release_name=rasqberry-v${VERSION_NUM}" >> $GITHUB_OUTPUT
          elif [[ "$BRANCH_NAME" == "beta" ]]; then
            echo "type=beta" >> $GITHUB_OUTPUT
            echo "stream=beta" >> $GITHUB_OUTPUT
            echo "name=RasQberry Two Beta (64-bit)" >> $GITHUB_OUTPUT
            echo "description=RasQberry beta release with latest features (may be unstable)" >> $GITHUB_OUTPUT
            echo "tag=${VERSION_NUM}" >> $GITHUB_OUTPUT
            echo "release_name=rasqberry-${VERSION_NUM}" >> $GITHUB_OUTPUT
          else
            echo "type=dev" >> $GITHUB_OUTPUT
            echo "stream=dev" >> $GITHUB_OUTPUT
            echo "name=RasQberry Two Dev (64-bit)" >> $GITHUB_OUTPUT
            echo "description=RasQberry development build with cutting-edge features (unstable)" >> $GITHUB_OUTPUT
            echo "tag=${VERSION_NUM}" >> $GITHUB_OUTPUT
            echo "release_name=rasqberry-${VERSION_NUM}" >> $GITHUB_OUTPUT
          fi
          
      - name: Generate RQB-images.json
        run: |
          # Generate single-entry RQB-images.json for THIS branch only
          # Consolidation into multi-stream file happens in separate workflow

          VERSION_TYPE="${{ steps.version-type.outputs.type }}"
          echo "Generating single-entry RQB-images.json for ${VERSION_TYPE} stream..."

          cat > RQB-images.json << EOF
          {
            "imager": {
              "latest_version": "1.8.5",
              "url": "https://www.raspberrypi.com/software/"
            },
            "os_list": [
              {
                "name": "${{ steps.version-type.outputs.name }}",
                "description": "${{ steps.version-type.outputs.description }}",
                "icon": "https://rasqberry.org/Artwork/RasQberry 2 Logo Cube 64x64.png",
                "url": "${{ steps.metadata.outputs.download_url }}",
                "extract_size": ${{ steps.metadata.outputs.extract_size }},
                "extract_sha256": "${{ steps.metadata.outputs.extract_sha256 }}",
                "image_download_size": ${{ steps.metadata.outputs.compressed_size }},
                "release_date": "${{ steps.metadata.outputs.release_date }}",
                "init_format": "systemd",
                "devices": ["pi5-64bit", "pi4-64bit"],
                "website": "https://rasqberry.org"
              }
            ]
          }
          EOF

          echo "Generated RQB-images.json with single ${VERSION_TYPE} entry:"
          cat RQB-images.json

      - name: Generate RQB-releases.json
        run: |
          # RQB-releases.json tracks the latest release per stream (stable/beta/dev)
          # This enables stable URLs for the latest release from each stream

          STREAM="${{ steps.version-type.outputs.stream }}"
          TAG="${{ steps.version-type.outputs.tag }}"
          RELEASE_NAME="${{ steps.version-type.outputs.release_name }}"

          echo "Generating RQB-releases.json for stream: $STREAM"

          # Try to fetch existing RQB-releases.json from gh-pages
          if curl -s -f "https://raw.githubusercontent.com/${{ github.repository }}/gh-pages/RQB-releases.json" -o existing-releases.json; then
            echo "✓ Found existing RQB-releases.json on gh-pages, using as base"
            cp existing-releases.json RQB-releases.json
          else
            echo "ℹ No existing RQB-releases.json found, creating fresh file"
            cat > RQB-releases.json << 'EOF'
          {
            "generated": "",
            "streams": {}
          }
          EOF
          fi

          # Use Python to update the JSON (handles JSON parsing properly)
          python3 << 'PYTHON_SCRIPT'
          import json
          from datetime import datetime, timezone

          # Load existing JSON
          with open('RQB-releases.json', 'r') as f:
              data = json.load(f)

          stream = "${{ steps.version-type.outputs.stream }}"

          # Create the stream entry with all metadata
          stream_entry = {
              "tag": "${{ steps.version-type.outputs.tag }}",
              "name": "${{ steps.version-type.outputs.release_name }}",
              "image_url": "${{ steps.metadata.outputs.download_url }}",
              "release_url": "https://github.com/${{ github.repository }}/releases/tag/${{ steps.version-type.outputs.tag }}",
              "release_date": "${{ steps.metadata.outputs.release_date }}",
              "image_download_size": ${{ steps.metadata.outputs.compressed_size }},
              "extract_size": ${{ steps.metadata.outputs.extract_size }},
              "extract_sha256": "${{ steps.metadata.outputs.extract_sha256 }}"
          }

          # Update timestamp and stream
          data['generated'] = datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')

          if 'streams' not in data:
              data['streams'] = {}

          data['streams'][stream] = stream_entry

          print(f"Updated '{stream}' stream:")
          print(f"  Tag: {stream_entry['tag']}")
          print(f"  Image URL: {stream_entry['image_url']}")

          # Write updated JSON with pretty formatting
          with open('RQB-releases.json', 'w') as f:
              json.dump(data, f, indent=2)

          print(f"\nRQB-releases.json now contains {len(data['streams'])} stream(s): {', '.join(data['streams'].keys())}")
          PYTHON_SCRIPT

          echo ""
          echo "=== Generated RQB-releases.json ==="
          cat RQB-releases.json
          echo ""

      - name: Commit JSON files to current branch
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"

          # Pull latest changes to avoid conflicts
          git pull origin ${{ github.ref_name }} || echo "No remote changes to pull"

          # Commit both JSON files to current branch
          git add RQB-images.json RQB-releases.json
          git commit -m "Generate RQB-images.json and RQB-releases.json for ${{ steps.version-type.outputs.type }} release ${{ steps.metadata.outputs.version_num }}"
          git push origin ${{ github.ref_name }}
          echo "Generated RQB-images.json and RQB-releases.json in current branch"

      - name: Upload JSON files to release
        uses: softprops/action-gh-release@v2.4.2
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          tag_name: ${{ github.ref == 'refs/heads/main' && format('v{0}', needs.rasqberry-push-version-number.outputs.version_num) || needs.rasqberry-push-version-number.outputs.version_num }}
          files: |
            RQB-images.json
            RQB-releases.json

      # RQB-releases.json is ALWAYS pushed to gh-pages (required for cross-branch consolidation)
      # RQB-images.json is only pushed when publish_json is true (Pi Imager compatibility)
      - name: Upload RQB-releases.json to gh-pages (always)
        run: |
          echo "Uploading RQB-releases.json to gh-pages (required for stream consolidation)..."

          # Fetch gh-pages branch
          git fetch origin gh-pages || echo "No gh-pages branch found"
          if git show-ref --verify --quiet refs/remotes/origin/gh-pages; then
            # Save the generated file before switching branches
            cp RQB-releases.json /tmp/RQB-releases.json

            git checkout gh-pages

            # Copy the generated RQB-releases.json
            cp /tmp/RQB-releases.json RQB-releases.json

            # Commit and push to gh-pages
            git add RQB-releases.json
            git commit -m "Update RQB-releases.json with ${{ steps.version-type.outputs.stream }} stream from ${{ steps.version-type.outputs.type }} release ${{ steps.metadata.outputs.version_num }}" || echo "No changes to commit"
            git push origin gh-pages
            echo "✓ RQB-releases.json uploaded to gh-pages"

            # Switch back to original branch for subsequent steps
            git checkout ${{ github.ref_name }}
          else
            echo "⚠ No gh-pages branch exists - RQB-releases.json consolidation requires gh-pages branch"
            echo "  Create gh-pages branch or run with publish_json=true on first build"
          fi

      # Trigger consolidation workflow to merge all branches into gh-pages
      - name: Trigger JSON consolidation workflow
        run: |
          echo "Triggering consolidation workflow to update gh-pages..."
          gh workflow run consolidate-json.yaml \
            --ref gh-pages \
            -f trigger_source="${{ github.ref_name }}" \
            -f trigger_type="${{ steps.version-type.outputs.type }}"
          echo "✓ Consolidation workflow triggered"
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

